# Chat Agent con LLM - Laboratorio S12

AplicaciÃ³n web de chat inteligente que integra un Large Language Model (LLM) mediante la API de OpenAI, implementada con Spring Boot y dockerizada para facilitar su despliegue.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un agente de chat conversacional que:
- Se comunica con la API de OpenAI (GPT-3.5-turbo)
- Mantiene historial de conversaciones en base de datos SQLite
- Proporciona interfaz web responsive con Thymeleaf
- EstÃ¡ completamente dockerizado para portabilidad

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Backend**: Spring Boot 3.4.0
- **Frontend**: Thymeleaf, HTML5, CSS3, JavaScript
- **Base de datos**: SQLite con JPA/Hibernate
- **LLM**: OpenAI GPT-3.5-turbo
- **ContainerizaciÃ³n**: Docker y Docker Compose
- **Java**: JDK 21

## ğŸ“ Estructura del Proyecto

chat-agent/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ main/
â”‚ â”‚ â”œâ”€â”€ java/com/example/chatagent/
â”‚ â”‚ â”‚ â”œâ”€â”€ controller/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ ChatController.java
â”‚ â”‚ â”‚ â”œâ”€â”€ service/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ ChatService.java
â”‚ â”‚ â”‚ â”œâ”€â”€ entity/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ Conversation.java
â”‚ â”‚ â”‚ â”œâ”€â”€ repository/
â”‚ â”‚ â”‚ â”‚ â””â”€â”€ ConversationRepository.java
â”‚ â”‚ â”‚ â””â”€â”€ ChatAgentApplication.java
â”‚ â”‚ â””â”€â”€ resources/
â”‚ â”‚ â”œâ”€â”€ application.properties
â”‚ â”‚ â”œâ”€â”€ templates/
â”‚ â”‚ â”‚ â””â”€â”€ chat.html
â”‚ â”‚ â””â”€â”€ static/js/
â”‚ â”‚ â””â”€â”€ chat.js
â”œâ”€â”€ data/
â”‚ â””â”€â”€ chat.db
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .dockerignore
â””â”€â”€ pom.xml


## ğŸš€ InstalaciÃ³n y EjecuciÃ³n

### Requisitos Previos

- Java 21+
- Maven 3.6+
- Docker y Docker Compose
- API Key de OpenAI

### ConfiguraciÃ³n

1. **Clonar el repositorio**:
   git clone <tu-repositorio>
   cd chat-agent
2. **Configurar API Key**:
   Editar `src/main/resources/application.properties`:
   openai.api.key=tu-api-key-aqui

### EjecuciÃ³n Local
Compilar
./mvnw clean package

Ejecutar
./mvnw spring-boot:run

Acceder a: http://localhost:8080

### EjecuciÃ³n con Docker
Construir imagen
docker-compose build

Ejecutar contenedor
docker-compose up

Ejecutar en segundo plano
docker-compose up -d

Detener
docker-compose down

## ğŸ¯ Funcionalidades

### 1. Chat Conversacional
- Interfaz web intuitiva y responsive
- EnvÃ­o de mensajes en tiempo real
- Respuestas generadas por GPT-3.5-turbo

### 2. Persistencia de Datos
- Historial completo de conversaciones
- Base de datos SQLite embebida
- Consultas por ID de usuario

### 3. Arquitectura de Microservicios
- SeparaciÃ³n clara de responsabilidades (MVC)
- Repository pattern con Spring Data JPA
- Service layer para lÃ³gica de negocio

### 4. DockerizaciÃ³n
- Imagen multi-stage para optimizaciÃ³n
- VolÃºmenes persistentes para datos
- Variables de entorno configurables

## ğŸ“Š Endpoints API

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| GET | `/` | Interfaz principal del chat |
| POST | `/chat/send` | Enviar mensaje al LLM |
| GET | `/chat/history` | Obtener historial de conversaciÃ³n |

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

En `docker-compose.yml`:
environment:

SPRING_DATASOURCE_URL=jdbc:sqlite:/app/data/chat.db

OPENAI_API_KEY=tu-api-key

OPENAI_MODEL=gpt-3.5-turbo

### Base de Datos

SQLite configurado en `application.properties`:
spring.datasource.url=jdbc:sqlite:chat.db
spring.jpa.hibernate.ddl-auto=update

Ejecutar tests
./mvnw test

Con cobertura
./mvnw clean verify

## ğŸ“ Conclusiones

1. **IntegraciÃ³n exitosa** de LLM en aplicaciÃ³n web mediante API de OpenAI
2. **Arquitectura robusta** basada en patrones MVC y Repository
3. **Persistencia efectiva** de conversaciones con SQLite/JPA
4. **DockerizaciÃ³n completa** garantiza portabilidad y fÃ¡cil despliegue
5. **Experiencia de usuario** fluida con interfaz responsive

## ğŸ‘¨â€ğŸ’» Autor

**JosÃ© Carlos Vitorino Condori**
- TECSUP Arequipa
- Desarrollo de Software
- Laboratorio S12 - 2025

## ğŸ“„ Licencia

Este proyecto fue desarrollado con fines acadÃ©micos para el curso de Desarrollo de Software en TECSUP.

## ğŸ”— Arquitectura de Microservicios

### Microservicio 1: Chat Agent (Spring Boot)
- **Lenguaje**: Java 21
- **Framework**: Spring Boot 3.4.0
- **Puerto**: 8080
- **Responsabilidades**:
    - Interfaz web con Thymeleaf
    - GestiÃ³n de conversaciones
    - Persistencia en SQLite
    - IntegraciÃ³n con OpenAI GPT-3.5
    - OrquestaciÃ³n de llamadas al LLM Toy

### Microservicio 2: LLM Toy Service (Python)
- **Lenguaje**: Python 3.11
- **Framework**: FastAPI
- **Puerto**: 8001
- **Responsabilidades**:
    - Modelo de lenguaje simple con PyTorch
    - GeneraciÃ³n de tokens con probabilidades
    - CÃ¡lculo de logprobs y alternativas
    - API REST documentada con Swagger

### ComunicaciÃ³n entre Microservicios
Usuario â†’ Frontend (Thymeleaf)
â†“
Spring Boot (8080)
â†“
â”œâ”€â†’ OpenAI API (GPT-3.5)
â””â”€â†’ Python Service (8001)
â†“
PyTorch LLM Toy

## ğŸ³ Docker Compose

Los servicios estÃ¡n orquestados mediante Docker Compose:

services:

chat-agent (Spring Boot)

llm-toy-service (Python/FastAPI)

network: microservices-network (bridge)

### Comandos Docker

Construir ambos servicios
docker-compose build

Ejecutar en primer plano
docker-compose up

Ejecutar en segundo plano
docker-compose up -d

Ver logs en tiempo real
docker-compose logs -f

Ver estado de servicios
docker-compose ps

Detener servicios
docker-compose down

Reconstruir desde cero
docker-compose down --rmi all
docker-compose build --no-cache
docker-compose up

## ğŸ“¡ Endpoints API

### Spring Boot (Puerto 8080)

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| GET | `/` | Interfaz web del chat |
| POST | `/chat/send` | Enviar mensaje a GPT-3.5 |
| POST | `/chat/send-toy` | Enviar mensaje al LLM Toy |
| GET | `/chat/history` | Obtener historial |
| GET | `/chat/toy-status` | Estado del servicio Python |

### Python LLM Toy (Puerto 8001)

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| GET | `/` | Estado del servicio |
| GET | `/health` | Health check |
| POST | `/generate` | Generar tokens con probabilidades |
| GET | `/vocab` | Obtener vocabulario disponible |
| GET | `/docs` | DocumentaciÃ³n Swagger |

## ğŸ§ª Pruebas

### Probar el LLM Toy directamente

curl -X POST "http://localhost:8001/generate"
-H "Content-Type: application/json"
-d '{"prompt": "la suma de"}'

### Verificar salud de servicios

Spring Boot
curl http://localhost:8080/chat/toy-status

Python
curl http://localhost:8001/health
undefined
## ğŸ“ Conclusiones

### Laboratorio 12 (Base)
1. IntegraciÃ³n exitosa de LLM en aplicaciÃ³n web mediante API de OpenAI
2. Arquitectura robusta basada en patrones MVC y Repository
3. Persistencia efectiva de conversaciones con SQLite/JPA
4. DockerizaciÃ³n completa garantiza portabilidad
5. Experiencia de usuario fluida con interfaz responsive

### Laboratorio 13 (Microservicios)
1. **Arquitectura PolÃ­glota**: Se implementÃ³ exitosamente una arquitectura de microservicios utilizando dos lenguajes diferentes (Java y Python), demostrando que cada servicio puede usar el stack tecnolÃ³gico mÃ¡s apropiado para su funciÃ³n especÃ­fica. Spring Boot maneja la orquestaciÃ³n y presentaciÃ³n, mientras Python/PyTorch ejecuta el procesamiento de ML.

2. **ComunicaciÃ³n entre Microservicios**: La integraciÃ³n mediante REST API entre servicios dockerizados demuestra el desacoplamiento efectivo. El uso de Docker Networks permite que los contenedores se comuniquen por nombre de servicio sin exponer puertos innecesariamente al host.

3. **LLM de Juguete con PyTorch**: Se construyÃ³ un modelo de lenguaje simplificado que ilustra los conceptos fundamentales de los LLMs reales: embeddings, softmax para probabilidades, y sampling. Aunque es un modelo "toy", demuestra cÃ³mo calcular y visualizar las probabilidades de generaciÃ³n token por token, concepto crucial para entender GPT, BERT y otros transformers.

4. **Escalabilidad Horizontal**: La arquitectura basada en Docker Compose facilita escalar servicios independientemente. Si el LLM Toy necesita mÃ¡s recursos, se puede escalar solo ese servicio sin afectar el backend principal. Esto prepara el camino hacia orquestadores como Kubernetes.

5. **Observabilidad y Debugging**: FastAPI proporciona documentaciÃ³n Swagger automÃ¡tica (/docs), facilitando el testing y debugging. Los health checks permiten monitoreo del estado de servicios. Esta separaciÃ³n de concerns mejora la mantenibilidad y permite a equipos especializados trabajar en cada microservicio de forma independiente.

# Ver servicios corriendo
docker-compose ps

# Ver logs de ambos servicios
docker-compose logs

# Ver logs solo del servicio Python
docker-compose logs llm-toy-service

# Verificar salud del LLM Toy
curl http://localhost:8001/health

# Probar generaciÃ³n directa
curl -X POST "http://localhost:8001/generate" \
-H "Content-Type: application/json" \
-d '{"prompt": "la suma de"}'



